Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent should take actions in a dynamic environment in order to maximize a reward signal. Reinforcement learning is one of the three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
Reinforcement learning differs from supervised learning in not needing labelled input-output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead, the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the cumulative reward (the feedback of which might be incomplete or delayed). The search for this balance is known as the exploration–exploitation dilemma.

The environment is typically stated in the form of a Markov decision process (MDP), as many reinforcement learning algorithms use dynamic programming techniques. The main difference between classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process, and they target large MDPs where exact methods become infeasible.


## Principles
Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, RL is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in RL have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation (particularly in the absence of a mathematical model of the environment).
Basic reinforcement learning is modeled as a Markov decision process:

A set of environment and agent states (the state space), 
  
    
      
        
          
            S
          
        
      
    
    {\displaystyle {\mathcal {S}}}
  
;
A set of actions (the action space), 
  
    
      
        
          
            A
          
        
      
    
    {\displaystyle {\mathcal {A}}}
  
, of the agent;

  
    
      
        
          P
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
        =
        Pr
        (
        
          S
          
            t
            +
            1
          
        
        =
        
          s
          ′
        
        ∣
        
          S
          
            t
          
        
        =
        s
        ,
        
          A
          
            t
          
        
        =
        a
        )
      
    
    {\displaystyle P_{a}(s,s')=\Pr(S_{t+1}=s'\mid S_{t}=s,A_{t}=a)}
  
, the transition probability (at time 
  
    
      
        t
      
    
    {\displaystyle t}
  
) from state 
  
    
      
        s
      
    
    {\displaystyle s}
  
 to state 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
  
 under action 
  
    
      
        a
      
    
    {\displaystyle a}
  
.

  
    
      
        
          R
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
      
    
    {\displaystyle R_{a}(s,s')}
  
, the immediate reward after transition from 
  
    
      
        s
      
    
    {\displaystyle s}
  
 to 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
  
 under action 
  
    
      
        a
      
    
    {\displaystyle a}
  
.
The purpose of reinforcement learning is for the agent to learn an optimal (or near-optimal) policy that maximizes the reward function or other user-provided reinforcement signal that accumulates from immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals learn to adopt behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.
A basic reinforcement learning agent interacts with its environment in discrete time steps. At each time step t, the agent receives the current state 
  
    
      
        
          S
          
            t
          
        
      
    
    {\displaystyle S_{t}}
  
 and reward 
  
    
      
        
          R
          
            t
          
        
      
    
    {\displaystyle R_{t}}
  
. It then chooses an action 
  
    
      
        
          A
          
            t
          
        
      
    
    {\displaystyle A_{t}}
  
 from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state 
  
    
      
        
          S
          
            t
            +
            1
          
        
      
    
    {\displaystyle S_{t+1}}
  
 and the reward 
  
    
      
        
          R
          
            t
            +
            1
          
        
      
    
    {\displaystyle R_{t+1}}
  
 associated with the transition 
  
    
      
        (
        
          S
          
            t
          
        
        ,
        
          A
          
            t
          
        
        ,
        
          S
          
            t
            +
            1
          
        
        )
      
    
    {\displaystyle (S_{t},A_{t},S_{t+1})}
  
 is determined. The goal of a reinforcement learning agent is to learn a policy:

  
    
      
        π
        :
        
          
            S
          
        
        ×
        
          
            A
          
        
        →
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle \pi :{\mathcal {S}}\times {\mathcal {A}}\rightarrow [0,1]}
  
, 
  
    
      
        π
        (
        s
        ,
        a
        )
        =
        Pr
        (
        
          A
          
            t
          
        
        =
        a
        ∣
        
          S
          
            t
          
        
        =
        s
        )
      
    
    {\displaystyle \pi (s,a)=\Pr(A_{t}=a\mid S_{t}=s)}
  

that maximizes the expected cumulative reward.
Formulating the problem as a Markov decision process assumes the agent directly observes the current environmental state; in this case, the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.
When the agent's performance is compared to that of an agent that acts optimally, the difference in performance yields the notion of regret. In order to act near optimally, the agent must reason about long-term consequences of its actions (i.e., maximize future rewards), although the immediate reward associated with this might be negative.
Thus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage, robot control, photovoltaic generators, backgammon, checkers, Go (AlphaGo), and autonomous driving systems.
Two elements make reinforcement learning powerful: the use of samples to optimize performance, and the use of function approximation to deal with large environments. Thanks to these two key components, RL can be used in large environments in the following situations:

A model of the environment is known, but an analytic solution is not available;
Only a simulation model of the environment is given (the subject of simulation-based optimization);
The only way to collect information about the environment is to interact with it.
The first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.

## Exploration
The exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).
Reinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.
One such method is 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
-greedy, where 
  
    
      
        0
        <
        ε
        <
        1
      
    
    {\displaystyle 0<\varepsilon <1}
  
 is a parameter controlling the amount of exploration vs. exploitation. With probability 
  
    
      
        1
        −
        ε
      
    
    {\displaystyle 1-\varepsilon }
  
, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
, exploration is chosen, and the action is chosen uniformly at random. 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
  
 is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.

## Algorithms for control learning
Even if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.

## Theory
Both the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.
Efficient exploration of Markov decision processes is given in Burnetas and Katehakis (1997). Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.
For incremental algorithms, asymptotic convergence issues have been settled. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).

## Research
Research topics include:

actor-critic architecture
actor-critic-scenery architecture
adaptive methods that work with fewer (or no) parameters under a large number of conditions
bug detection in software projects
continuous learning
combinations with logic-based frameworks
exploration in large Markov decision processes
human feedback
interaction between implicit and explicit learning in skill acquisition
intrinsic motivation which differentiates information-seeking, curiosity-type behaviours from task-dependent goal-directed behaviours large-scale empirical evaluations
large (or continuous) action spaces
modular and hierarchical reinforcement learning
multiagent/distributed reinforcement learning is a topic of interest. Applications are expanding.
occupant-centric control
optimization of computing resources
partial information (e.g., using predictive state representation)
reward function based on maximising novel information
sample-based planning (e.g., based on Monte Carlo tree search).
securities trading
transfer learning
TD learning modeling dopamine-based learning in the brain. Dopaminergic projections from the substantia nigra to the basal ganglia function are the prediction error.
value-function and policy search methods

## Comparison of key algorithms
The following table lists the key algorithms for learning a policy depending on several criteria:

The algorithm can be on-policy (it performs policy updates using trajectories sampled via the current policy) or off-policy.
The action space may be discrete (e.g. the action space could be "going up", "going left", "going right", "going down", "stay") or continuous (e.g. moving the arm with a given angle).
The state space may be discrete (e.g. the agent could be in a cell in a grid) or continuous (e.g. the agent could be located at a given position in the plane).

## Statistical comparison of reinforcement learning algorithms
Efficient comparison of RL algorithms is essential for research, deployment and monitoring of RL systems. To compare different algorithms on a given environment, an agent can be trained for each algorithm. Since the performance is sensitive to implementation details, all algorithms should be implemented as closely as possible to each other. After the training is finished, the agents can be run on a sample of test episodes, and their scores (returns) can be compared. Since episodes are typically assumed to be i.i.d, standard statistical tools can be used for hypothesis testing, such as T-test and permutation test. This requires to accumulate all the rewards within an episode into a single number—the episodic return. However, this causes a loss of information, as different time-steps are averaged together, possibly with different levels of noise. Whenever the noise level varies across the episode, the statistical power can be improved significantly, by weighting the rewards according to their estimated noise.

## See also


## References


## Further reading
Annaswamy, Anuradha M. (3 May 2023). "Adaptive Control and Intersections with Reinforcement Learning". Annual Review of Control, Robotics, and Autonomous Systems. 6 (1): 65–93. doi:10.1146/annurev-control-062922-090153. ISSN 2573-5144. S2CID 255702873.
Auer, Peter; Jaksch, Thomas; Ortner, Ronald (2010). "Near-optimal regret bounds for reinforcement learning". Journal of Machine Learning Research. 11: 1563–1600.
Bertsekas, Dimitri P. (2023) [2019]. REINFORCEMENT LEARNING AND OPTIMAL CONTROL (1st ed.). Athena Scientific. ISBN 978-1-886-52939-7.
Busoniu, Lucian; Babuska, Robert; De Schutter, Bart; Ernst, Damien (2010). Reinforcement Learning and Dynamic Programming using Function Approximators. Taylor & Francis CRC Press. ISBN 978-1-4398-2108-4.
François-Lavet, Vincent; Henderson, Peter; Islam, Riashat; Bellemare, Marc G.; Pineau, Joelle (2018). "An Introduction to Deep Reinforcement Learning". Foundations and Trends in Machine Learning. 11 (3–4): 219–354. arXiv:1811.12560. Bibcode:2018arXiv181112560F. doi:10.1561/2200000071. S2CID 54434537.
Li, Shengbo Eben (2023). Reinforcement Learning for Sequential Decision and Optimal Control (1st ed.). Springer Verlag, Singapore. doi:10.1007/978-981-19-7784-8. ISBN 978-9-811-97783-1.
Powell, Warren (2011). Approximate dynamic programming: solving the curses of dimensionality. Wiley-Interscience. Archived from the original on 2016-07-31. Retrieved 2010-09-08.
Sutton, Richard S. (1988). "Learning to predict by the method of temporal differences". Machine Learning. 3: 9–44. doi:10.1007/BF00115009.
Sutton, Richard S.; Barto, Andrew G. (2018) [1998]. Reinforcement Learning: An Introduction (2nd ed.). MIT Press. ISBN 978-0-262-03924-6.
Szita, Istvan; Szepesvari, Csaba (2010). "Model-based Reinforcement Learning with Nearly Tight Exploration Complexity Bounds" (PDF). ICML 2010. Omnipress. pp. 1031–1038. Archived from the original (PDF) on 2010-07-14.

## External links
Dissecting Reinforcement Learning Series of blog post on reinforcement learning with Python code
A (Long) Peek into Reinforcement Learning